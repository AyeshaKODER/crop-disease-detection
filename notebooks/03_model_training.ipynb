{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753ac9d7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # 03 - Model Training & Experimentation\n",
    "# This notebook tests different model architectures and hyperparameters\n",
    "\n",
    "# %%\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import custom modules\n",
    "import sys\n",
    "sys.path.append('src')\n",
    "from model import ModelBuilder\n",
    "from data_loader import PlantVillageDataLoader\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "# %%\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"GPU Available:\", len(tf.config.list_physical_devices('GPU')) > 0)\n",
    "print(\"GPU Devices:\", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1. Load Data\n",
    "\n",
    "# %%\n",
    "# Initialize data loader\n",
    "loader = PlantVillageDataLoader(\n",
    "    raw_data_path='data/raw/PlantVillage',\n",
    "    processed_data_path='data/processed',\n",
    "    img_size=(224, 224),\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Create data generators\n",
    "train_gen, val_gen, test_gen = loader.create_data_generators(\n",
    "    batch_size=32,\n",
    "    augment_train=True\n",
    ")\n",
    "\n",
    "# Load class names\n",
    "with open('data/processed/class_names.json', 'r') as f:\n",
    "    class_names = json.load(f)\n",
    "\n",
    "print(f\"\\n✅ Data loaded:\")\n",
    "print(f\"   Train: {train_gen.samples} images\")\n",
    "print(f\"   Val: {val_gen.samples} images\")\n",
    "print(f\"   Test: {test_gen.samples} images\")\n",
    "print(f\"   Classes: {len(class_names)}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2. Experiment 1: Compare Model Architectures\n",
    "\n",
    "# %%\n",
    "def quick_train_test(model_name, epochs=5):\n",
    "    \"\"\"\n",
    "    Quickly train model for comparison\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Testing: {model_name.upper()}\")\n",
    "    print('='*70)\n",
    "    \n",
    "    # Build model\n",
    "    model = ModelBuilder.build(\n",
    "        model_name=model_name,\n",
    "        input_shape=(224, 224, 3),\n",
    "        num_classes=len(class_names)\n",
    "    )\n",
    "    \n",
    "    # Compile\n",
    "    ModelBuilder.compile_model(model, learning_rate=0.001)\n",
    "    \n",
    "    # Train for few epochs\n",
    "    history = model.fit(\n",
    "        train_gen,\n",
    "        epochs=epochs,\n",
    "        validation_data=val_gen,\n",
    "        steps_per_epoch=100,  # Limited for quick test\n",
    "        validation_steps=50,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    val_loss, val_acc, val_top3, val_precision, val_recall = model.evaluate(\n",
    "        val_gen,\n",
    "        steps=50,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    results = {\n",
    "        'model': model_name,\n",
    "        'params': model.count_params(),\n",
    "        'val_accuracy': val_acc,\n",
    "        'val_top3': val_top3,\n",
    "        'val_loss': val_loss,\n",
    "        'training_history': history.history\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Test Different Architectures\n",
    "\n",
    "# %%\n",
    "# IMPORTANT: This will take time! Comment out models you don't want to test\n",
    "\n",
    "models_to_test = [\n",
    "    'custom_cnn',\n",
    "    'mobilenet_v2',\n",
    "    # 'efficientnet_b0',  # Uncomment to test\n",
    "    # 'resnet50'  # Uncomment to test\n",
    "]\n",
    "\n",
    "comparison_results = []\n",
    "\n",
    "for model_name in models_to_test:\n",
    "    try:\n",
    "        result = quick_train_test(model_name, epochs=5)\n",
    "        comparison_results.append(result)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error with {model_name}: {e}\")\n",
    "\n",
    "# %%\n",
    "# Compare results\n",
    "df_comparison = pd.DataFrame([\n",
    "    {\n",
    "        'Model': r['model'],\n",
    "        'Parameters': f\"{r['params']:,}\",\n",
    "        'Val Accuracy': f\"{r['val_accuracy']:.4f}\",\n",
    "        'Val Top-3': f\"{r['val_top3']:.4f}\",\n",
    "        'Val Loss': f\"{r['val_loss']:.4f}\"\n",
    "    }\n",
    "    for r in comparison_results\n",
    "])\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL COMPARISON (5 epochs)\")\n",
    "print(\"=\"*70)\n",
    "print(df_comparison.to_string(index=False))\n",
    "\n",
    "# %%\n",
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "models = [r['model'] for r in comparison_results]\n",
    "accuracies = [r['val_accuracy'] for r in comparison_results]\n",
    "params = [r['params'] / 1e6 for r in comparison_results]  # in millions\n",
    "\n",
    "# Accuracy comparison\n",
    "axes[0].bar(models, accuracies, color=['skyblue', 'coral', 'lightgreen', 'plum'][:len(models)])\n",
    "axes[0].set_ylabel('Validation Accuracy', fontsize=12)\n",
    "axes[0].set_title('Model Accuracy Comparison (5 epochs)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylim([0, 1])\n",
    "for i, v in enumerate(accuracies):\n",
    "    axes[0].text(i, v + 0.02, f'{v:.3f}', ha='center', fontweight='bold')\n",
    "\n",
    "# Parameters comparison\n",
    "axes[1].bar(models, params, color=['skyblue', 'coral', 'lightgreen', 'plum'][:len(models)])\n",
    "axes[1].set_ylabel('Parameters (Millions)', fontsize=12)\n",
    "axes[1].set_title('Model Size Comparison', fontsize=14, fontweight='bold')\n",
    "for i, v in enumerate(params):\n",
    "    axes[1].text(i, v + 0.1, f'{v:.1f}M', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/model_architecture_comparison.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3. Experiment 2: Learning Rate Tuning\n",
    "\n",
    "# %%\n",
    "def test_learning_rate(lr, epochs=10):\n",
    "    \"\"\"Test different learning rates\"\"\"\n",
    "    print(f\"\\n Testing LR: {lr}\")\n",
    "    \n",
    "    model = ModelBuilder.build('mobilenet_v2', num_classes=len(class_names))\n",
    "    ModelBuilder.compile_model(model, learning_rate=lr)\n",
    "    \n",
    "    history = model.fit(\n",
    "        train_gen,\n",
    "        epochs=epochs,\n",
    "        validation_data=val_gen,\n",
    "        steps_per_epoch=100,\n",
    "        validation_steps=50,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    return history.history\n",
    "\n",
    "# %%\n",
    "# Test different learning rates\n",
    "learning_rates = [0.0001, 0.0005, 0.001, 0.005]\n",
    "lr_results = {}\n",
    "\n",
    "print(\"Testing learning rates...\")\n",
    "for lr in learning_rates:\n",
    "    lr_results[lr] = test_learning_rate(lr, epochs=5)\n",
    "\n",
    "# %%\n",
    "# Plot learning rate comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for lr in learning_rates:\n",
    "    ax1.plot(lr_results[lr]['accuracy'], label=f'LR={lr}', marker='o')\n",
    "    ax2.plot(lr_results[lr]['val_accuracy'], label=f'LR={lr}', marker='o')\n",
    "\n",
    "ax1.set_title('Training Accuracy vs Learning Rate', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.set_title('Validation Accuracy vs Learning Rate', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Validation Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/learning_rate_comparison.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Find best LR\n",
    "best_lr = max(learning_rates, key=lambda lr: max(lr_results[lr]['val_accuracy']))\n",
    "print(f\"\\n✅ Best Learning Rate: {best_lr}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4. Experiment 3: Batch Size Impact\n",
    "\n",
    "# %%\n",
    "def test_batch_size(batch_size, epochs=5):\n",
    "    \"\"\"Test different batch sizes\"\"\"\n",
    "    print(f\"\\nTesting Batch Size: {batch_size}\")\n",
    "    \n",
    "    # Create new generators with different batch size\n",
    "    train_gen_temp, val_gen_temp, _ = loader.create_data_generators(\n",
    "        batch_size=batch_size,\n",
    "        augment_train=True\n",
    "    )\n",
    "    \n",
    "    model = ModelBuilder.build('mobilenet_v2', num_classes=len(class_names))\n",
    "    ModelBuilder.compile_model(model, learning_rate=0.001)\n",
    "    \n",
    "    history = model.fit(\n",
    "        train_gen_temp,\n",
    "        epochs=epochs,\n",
    "        validation_data=val_gen_temp,\n",
    "        steps_per_epoch=50,\n",
    "        validation_steps=25,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    return history.history\n",
    "\n",
    "# %%\n",
    "# Test different batch sizes (if you have enough memory)\n",
    "batch_sizes = [16, 32]  # Add 64, 128 if you have GPU with enough memory\n",
    "batch_results = {}\n",
    "\n",
    "print(\"Testing batch sizes...\")\n",
    "for bs in batch_sizes:\n",
    "    try:\n",
    "        batch_results[bs] = test_batch_size(bs, epochs=5)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Batch size {bs} failed: {e}\")\n",
    "\n",
    "# %%\n",
    "# Plot batch size comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for bs in batch_results.keys():\n",
    "    axes[0].plot(batch_results[bs]['loss'], label=f'BS={bs}', marker='o')\n",
    "    axes[1].plot(batch_results[bs]['val_loss'], label=f'BS={bs}', marker='o')\n",
    "\n",
    "axes[0].set_title('Training Loss vs Batch Size', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].set_title('Validation Loss vs Batch Size', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Validation Loss')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/batch_size_comparison.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 5. Full Training: Best Configuration\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING FINAL MODEL (BEST CONFIGURATION)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Best configuration from experiments\n",
    "BEST_CONFIG = {\n",
    "    'model': 'mobilenet_v2',\n",
    "    'learning_rate': 0.001,\n",
    "    'batch_size': 32,\n",
    "    'epochs': 20  # Increase to 30-50 for production\n",
    "}\n",
    "\n",
    "print(\"\\nBest Configuration:\")\n",
    "for k, v in BEST_CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "# %%\n",
    "# Build best model\n",
    "final_model = ModelBuilder.build(\n",
    "    model_name=BEST_CONFIG['model'],\n",
    "    input_shape=(224, 224, 3),\n",
    "    num_classes=len(class_names),\n",
    "    trainable_layers=20,\n",
    "    dropout_rate=0.5\n",
    ")\n",
    "\n",
    "ModelBuilder.compile_model(final_model, learning_rate=BEST_CONFIG['learning_rate'])\n",
    "\n",
    "# %%\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# %%\n",
    "# Train final model\n",
    "print(\"\\n🚀 Starting training...\")\n",
    "print(\"⏰ This will take time! Go grab a coffee ☕\")\n",
    "\n",
    "history = final_model.fit(\n",
    "    train_gen,\n",
    "    epochs=BEST_CONFIG['epochs'],\n",
    "    validation_data=val_gen,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n✅ Training complete!\")\n",
    "\n",
    "# %%\n",
    "# Plot training history\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Accuracy\n",
    "axes[0, 0].plot(history.history['accuracy'], label='Train', linewidth=2)\n",
    "axes[0, 0].plot(history.history['val_accuracy'], label='Validation', linewidth=2)\n",
    "axes[0, 0].set_title('Model Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Accuracy')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Loss\n",
    "axes[0, 1].plot(history.history['loss'], label='Train', linewidth=2)\n",
    "axes[0, 1].plot(history.history['val_loss'], label='Validation', linewidth=2)\n",
    "axes[0, 1].set_title('Model Loss', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Loss')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Top-3 Accuracy\n",
    "axes[1, 0].plot(history.history['top_3_accuracy'], label='Train', linewidth=2)\n",
    "axes[1, 0].plot(history.history['val_top_3_accuracy'], label='Validation', linewidth=2)\n",
    "axes[1, 0].set_title('Top-3 Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Top-3 Accuracy')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Precision and Recall\n",
    "axes[1, 1].plot(history.history['precision'], label='Train Precision', linewidth=2)\n",
    "axes[1, 1].plot(history.history['val_precision'], label='Val Precision', linewidth=2)\n",
    "axes[1, 1].plot(history.history['recall'], label='Train Recall', linewidth=2, linestyle='--')\n",
    "axes[1, 1].plot(history.history['val_recall'], label='Val Recall', linewidth=2, linestyle='--')\n",
    "axes[1, 1].set_title('Precision & Recall', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Score')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Training History - Final Model', fontsize=16, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/final_training_curves.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 6. Model Evaluation on Test Set\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EVALUATING ON TEST SET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Evaluate\n",
    "test_loss, test_acc, test_top3, test_precision, test_recall = final_model.evaluate(\n",
    "    test_gen,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(f\"\\n📊 Test Results:\")\n",
    "print(f\"   Accuracy: {test_acc:.4f}\")\n",
    "print(f\"   Top-3 Accuracy: {test_top3:.4f}\")\n",
    "print(f\"   Loss: {test_loss:.4f}\")\n",
    "print(f\"   Precision: {test_precision:.4f}\")\n",
    "print(f\"   Recall: {test_recall:.4f}\")\n",
    "\n",
    "# %%\n",
    "# Get predictions for confusion matrix\n",
    "print(\"\\nGenerating predictions...\")\n",
    "y_pred_probs = final_model.predict(test_gen, verbose=1)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "y_true = test_gen.classes\n",
    "\n",
    "# %%\n",
    "# Classification Report\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CLASSIFICATION REPORT\")\n",
    "print(\"=\"*70)\n",
    "report = classification_report(y_true, y_pred, target_names=class_names, digits=4)\n",
    "print(report)\n",
    "\n",
    "# %%\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "plt.figure(figsize=(20, 18))\n",
    "sns.heatmap(\n",
    "    cm_normalized,\n",
    "    annot=False,\n",
    "    cmap='Blues',\n",
    "    xticklabels=class_names,\n",
    "    yticklabels=class_names,\n",
    "    cbar_kws={'label': 'Normalized Count'}\n",
    ")\n",
    "plt.title('Confusion Matrix (Normalized)', fontsize=16, fontweight='bold')\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.xticks(rotation=90, ha='right', fontsize=8)\n",
    "plt.yticks(rotation=0, fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/final_confusion_matrix.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 7. Analyze Best and Worst Performing Classes\n",
    "\n",
    "# %%\n",
    "# Per-class accuracy\n",
    "report_dict = classification_report(y_true, y_pred, target_names=class_names, output_dict=True)\n",
    "\n",
    "class_metrics = []\n",
    "for class_name in class_names:\n",
    "    if class_name in report_dict:\n",
    "        class_metrics.append({\n",
    "            'class': class_name,\n",
    "            'precision': report_dict[class_name]['precision'],\n",
    "            'recall': report_dict[class_name]['recall'],\n",
    "            'f1-score': report_dict[class_name]['f1-score'],\n",
    "            'support': report_dict[class_name]['support']\n",
    "        })\n",
    "\n",
    "df_metrics = pd.DataFrame(class_metrics)\n",
    "df_metrics = df_metrics.sort_values('f1-score', ascending=False)\n",
    "\n",
    "# %%\n",
    "# Top 10 best classes\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TOP 10 BEST PERFORMING CLASSES\")\n",
    "print(\"=\"*70)\n",
    "print(df_metrics.head(10).to_string(index=False))\n",
    "\n",
    "# %%\n",
    "# Bottom 10 worst classes\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BOTTOM 10 WORST PERFORMING CLASSES\")\n",
    "print(\"=\"*70)\n",
    "print(df_metrics.tail(10).to_string(index=False))\n",
    "\n",
    "# %%\n",
    "# Visualize per-class performance\n",
    "fig, ax = plt.subplots(figsize=(12, max(10, len(class_names) * 0.3)))\n",
    "\n",
    "x = np.arange(len(df_metrics))\n",
    "width = 0.25\n",
    "\n",
    "ax.barh(x - width, df_metrics['precision'], width, label='Precision', color='steelblue')\n",
    "ax.barh(x, df_metrics['recall'], width, label='Recall', color='coral')\n",
    "ax.barh(x + width, df_metrics['f1-score'], width, label='F1-Score', color='mediumseagreen')\n",
    "\n",
    "ax.set_xlabel('Score', fontsize=12)\n",
    "ax.set_title('Per-Class Performance Metrics', fontsize=14, fontweight='bold')\n",
    "ax.set_yticks(x)\n",
    "ax.set_yticklabels(df_metrics['class'], fontsize=8)\n",
    "ax.legend()\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/per_class_performance.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 8. Analyze Misclassifications\n",
    "\n",
    "# %%\n",
    "# Find most confused pairs\n",
    "misclassified = cm.copy()\n",
    "np.fill_diagonal(misclassified, 0)\n",
    "\n",
    "# Top 10 confused pairs\n",
    "confused_pairs = []\n",
    "for i in range(len(class_names)):\n",
    "    for j in range(len(class_names)):\n",
    "        if i != j and misclassified[i, j] > 0:\n",
    "            confused_pairs.append({\n",
    "                'true': class_names[i],\n",
    "                'predicted': class_names[j],\n",
    "                'count': misclassified[i, j]\n",
    "            })\n",
    "\n",
    "df_confused = pd.DataFrame(confused_pairs).sort_values('count', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TOP 10 MOST CONFUSED CLASS PAIRS\")\n",
    "print(\"=\"*70)\n",
    "print(df_confused.head(10).to_string(index=False))\n",
    "\n",
    "# %%\n",
    "# Visualize top confused pairs\n",
    "top_confused = df_confused.head(10)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "y_pos = np.arange(len(top_confused))\n",
    "labels = [f\"{row['true'][:30]}\\n→ {row['predicted'][:30]}\" for _, row in top_confused.iterrows()]\n",
    "\n",
    "ax.barh(y_pos, top_confused['count'], color='salmon')\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(labels, fontsize=9)\n",
    "ax.set_xlabel('Number of Misclassifications', fontsize=12)\n",
    "ax.set_title('Top 10 Most Confused Class Pairs', fontsize=14, fontweight='bold')\n",
    "ax.invert_yaxis()\n",
    "\n",
    "for i, v in enumerate(top_confused['count']):\n",
    "    ax.text(v + 0.5, i, str(int(v)), va='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/confused_pairs.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 9. Save Model\n",
    "\n",
    "# %%\n",
    "# Save model\n",
    "model_path = Path('models/notebook_trained_model.h5')\n",
    "model_path.parent.mkdir(exist_ok=True)\n",
    "final_model.save(model_path)\n",
    "\n",
    "print(f\"\\n✅ Model saved to {model_path}\")\n",
    "print(f\"   Model size: {model_path.stat().st_size / (1024*1024):.2f} MB\")\n",
    "\n",
    "# %%\n",
    "# Save training history\n",
    "history_dict = {k: [float(v) for v in vals] for k, vals in history.history.items()}\n",
    "\n",
    "history_path = Path('results/notebook_training_history.json')\n",
    "with open(history_path, 'w') as f:\n",
    "    json.dump(history_dict, f, indent=2)\n",
    "\n",
    "print(f\"✅ Training history saved to {history_path}\")\n",
    "\n",
    "# %%\n",
    "# Save evaluation results\n",
    "eval_results = {\n",
    "    'test_accuracy': float(test_acc),\n",
    "    'test_top3_accuracy': float(test_top3),\n",
    "    'test_loss': float(test_loss),\n",
    "    'test_precision': float(test_precision),\n",
    "    'test_recall': float(test_recall),\n",
    "    'best_classes': df_metrics.head(5)['class'].tolist(),\n",
    "    'worst_classes': df_metrics.tail(5)['class'].tolist(),\n",
    "    'timestamp': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "eval_path = Path('results/notebook_evaluation.json')\n",
    "with open(eval_path, 'w') as f:\n",
    "    json.dump(eval_results, f, indent=2)\n",
    "\n",
    "print(f\"✅ Evaluation results saved to {eval_path}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 10. Summary & Recommendations\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING SUMMARY & RECOMMENDATIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "summary = f\"\"\"\n",
    "✅ FINAL MODEL PERFORMANCE:\n",
    "   • Test Accuracy: {test_acc:.2%}\n",
    "   • Top-3 Accuracy: {test_top3:.2%}\n",
    "   • Precision: {test_precision:.2%}\n",
    "   • Recall: {test_recall:.2%}\n",
    "\n",
    "✅ MODEL CONFIGURATION:\n",
    "   • Architecture: {BEST_CONFIG['model']}\n",
    "   • Learning Rate: {BEST_CONFIG['learning_rate']}\n",
    "   • Batch Size: {BEST_CONFIG['batch_size']}\n",
    "   • Epochs Trained: {len(history.history['loss'])}\n",
    "\n",
    "✅ BEST PERFORMING CLASSES:\n",
    "   {', '.join(df_metrics.head(3)['class'].tolist())}\n",
    "\n",
    "⚠️  CLASSES NEEDING IMPROVEMENT:\n",
    "   {', '.join(df_metrics.tail(3)['class'].tolist())}\n",
    "\n",
    "📊 NEXT STEPS:\n",
    "   1. If accuracy < 95%: Train longer (40-50 epochs)\n",
    "   2. Try EfficientNetB0 for +1-2% accuracy\n",
    "   3. Add more augmentation for struggling classes\n",
    "   4. Consider ensemble methods\n",
    "   5. Deploy model using streamlit app\n",
    "\n",
    "💡 PRODUCTION RECOMMENDATIONS:\n",
    "   • Use model quantization for mobile deployment\n",
    "   • Implement confidence thresholding (reject < 70%)\n",
    "   • Add Grad-CAM for explainability\n",
    "   • Monitor model drift in production\n",
    "\"\"\"\n",
    "\n",
    "print(summary)\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"🎉 MODEL TRAINING NOTEBOOK COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n✅ Model trained and evaluated\")\n",
    "print(\"✅ All visualizations saved to results/\")\n",
    "print(\"✅ Model ready for deployment\")\n",
    "print(\"\\n📍 Next Steps:\")\n",
    "print(\"   1. Test model: python src/predict.py --image examples/test.jpg\")\n",
    "print(\"   2. Run web app: streamlit run app/app.py\")\n",
    "print(\"   3. Push to GitHub\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
